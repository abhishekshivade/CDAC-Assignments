Q1.
An operating system (OS) is an interface between the computer hardware and the user. It manages software resources and computer hardware, while providing various services for computer programs. The operating system is essential software running on a computer, enabling it to work effectively.

Process management
This involves managing every process that the system hardware executes. The OS decides the order in which processes are performed, prioritizing them based on variables like efficiency and necessity.

Memory management
The operating system oversees your computer's memory, allocating space to processes when they run and deallocating them when they halt. Your computer's memory is a critical resource, and keeping it organized is key to maintaining your system's performance.

File systems management
The OS also manages your data files. It uses a systematic way to store, arrange, and retrieve files and data. Furthermore, it keeps track of all the data, ensuring that it doesn't get lost or overwritten accidentally.

Device management
An operating system manages all input, output, and storage devices. The system ensures that your devices run as effectively as possible and that their essential data is stored safely.

Security and privacy
Maintaining system security is crucial. The OS ensures the security of the system by limiting hardware access to trusted applications and programs. It uses methods like password protection and controlled user access to uphold privacy and data integrity.

Q2 
https://www.geeksforgeeks.org/difference-between-process-and-thread/

Q3
Virtual Memory is a storage scheme that provides user an illusion of having a very big main memory. This is done by treating a part of secondary memory as the main memory.

In modern word, virtual memory has become quite common these days. In this scheme, whenever some pages needs to be loaded in the main memory for the execution and the memory is not available for those many pages, then in that case, instead of stopping the pages from entering in the main memory, the OS search for the RAM area that are least used in the recent times or that are not referenced and copy that into the secondary memory to make the space for the new pages in the main memory.

Since all this procedure happens automatically, therefore it makes the computer feel like it is having the unlimited RAM.

Q4.
https://www.javatpoint.com/multiprogramming-vs-multiprocessing-vs-multitasking-vs-multithreading

Q5.
A file system is a method an operating system uses to store, organize, and manage files and directories on a storage device.

Q6.
A Deadlock is a situation where each of the computer process waits for a resource which is being assigned to some another process. In this situation, none of the process gets executed since the resource it needs, is held by some other process which is also waiting for some other resource to be released.

In the deadlock prevention process, the OS will prevent the deadlock from occurring by avoiding any one of the four conditions that caused the deadlock. If the OS can avoid any of the necessary conditions, a deadlock will not occur.

4.1. No Mutual Exclusion

It means more than one process can have access to a single resource at the same time. It’s impossible because if multiple processes access the same resource simultaneously, there will be chaos. Additionally, no process will be completed. So this is not feasible. Hence, the OS can’t avoid mutual exclusion.

4.2. No Hold and Wait

To avoid the hold and wait, there are many ways to acquire all the required resources before starting the execution. But this is also not feasible because a process will use a single resource at a time. Here, the resource utilization will be very less.

Before starting the execution, the process does not know how many resources would be required to complete it. In addition to that, the bus time, in which a process will complete and free the resource, is also unknown.

Another way is if a process is holding a resource and wants to have additional resources, then it must free the acquired resources. This way, we can avoid the hold and wait condition, but it can result in starvation.

4.3. Removal of No Preemption

One of the reasons that cause the deadlock is the no preemption. It means the CPU can’t take acquired resources from any process forcefully even though that process is in a waiting state. If we can remove the no preemption and forcefully take resources from a waiting process, we can avoid the deadlock. This is an implementable logic to avoid deadlock.

4.4. Removal of Circular Wait

In the circular wait, two processes are stuck in the waiting state for the resources which have been held by each other. To avoid the circular wait, we assign a numerical integer value to all resources, and a process has to access the resource in increasing or decreasing order.

If the process acquires resources in increasing order, it’ll only have access to the new additional resource if that resource has a higher integer value. And if that resource has a lesser integer value, it must free the acquired resource before taking the new resource and vice-versa for decreasing order.

Q7.
https://www.geeksforgeeks.org/difference-between-shell-and-kernel/

Q8.
Scheduling of processes/work is done to finish the work on time. CPU Scheduling is a process that allows one process to use the CPU while another process is delayed (in standby) due to unavailability of any resources such as I / O etc, thus making full use of the CPU. The purpose of CPU Scheduling is to make the system more efficient, faster, and fairer.

n Multiprogramming, if the long term scheduler picks more I/O bound processes then most of the time, the CPU remains idol. The task of Operating system is to optimize the utilization of resources.

If most of the running processes change their state from running to waiting then there may always be a possibility of deadlock in the system. Hence to reduce this overhead, the OS needs to schedule the jobs to get the optimal utilization of CPU and to avoid the possibility to deadlock.

Q9.
The Applications run in an area of memory known as user space. A system call connects to the operating system's kernel, which executes in kernel space. When an application creates a system call, it must first obtain permission from the kernel. It achieves this using an interrupt request, which pauses the current process and transfers control to the kernel.

If the request is permitted, the kernel performs the requested action, like creating or deleting a file. As input, the application receives the kernel's output. The application resumes the procedure after the input is received. When the operation is finished, the kernel returns the results to the application and then moves data from kernel space to user space in memory.

A simple system call may take few nanoseconds to provide the result, like retrieving the system date and time. A more complicated system call, such as connecting to a network device, may take a few seconds. Most operating systems launch a distinct kernel thread for each system call to avoid bottlenecks. Modern operating systems are multi-threaded, which means they can handle various system calls at the same time.

Q10.
The main purpose of device drivers is to provide abstraction by acting as a translator between a hardware device and the applications or operating systems that use it. Programmers can write higher-level application code independently of whatever specific hardware the end-user is using.

For example, a high-level application interacting with a serial port may have "send data" and "receive data" functions. A device driver implementing these functions would communicate to the particular serial port controller installed at a lower level on a user's computer.

The commands needed to control a 16550 UART are different from those needed to control an FTDI serial port converter. Still, each hardware-specific device driver abstracts these details into the same or similar software interface.

Q11.
In virtual memory systems, the page table plays a crucial role in bridging the gap between logical addresses used by programs and physical addresses recognized by hardware. Here's a breakdown of its key purposes:

1. Address Translation: The primary function of the page table is to translate virtual addresses generated by a program into their corresponding physical addresses in RAM. This translation happens in steps:

    Page number extraction: The virtual address is split into a page number and an offset within the page.
    Page table lookup: The page number is used to find the relevant entry in the page table.
    Frame number retrieval: This entry stores the frame number in RAM where the requested page resides.
    Physical address calculation: The frame number and the offset are combined to create the final physical address.

2. Memory Management: The page table facilitates efficient memory management by dividing both physical and virtual memory into fixed-size units called pages and frames, respectively. This approach offers several advantages:

    Flexibility: Enables multiple processes to share physical memory dynamically, maximizing resource utilization.
    Protection: Isolates memory of different processes, preventing interference and memory corruption.
    Demand Paging: Allows loading only the required pages into RAM as needed, keeping overall memory usage efficient.

3. Access Control and Protection: Page tables can enforce access control for each page, specifying permissions like read-only, read-write, or execute. This helps prevent unauthorized access and accidental modifications, enhancing system security.

4. Memory Sharing: By sharing page tables, processes can access common data structures without duplicating them in memory, further optimizing resource usage.

5. Paging Out and Swapping: When physical memory runs low, inactive pages can be swapped out to secondary storage (like a hard drive) and brought back when needed. The page table tracks these swapped-out pages to facilitate retrieval.

In essence, the page table acts as a translator and manager, enabling efficient and secure memory utilization in virtual memory systems.

Q12.
In computer science, thrashing occurs when a computer's virtual memory resources are overused, leading to a constant state of paging and page faults, inhibiting most application-level processing. It causes the performance of the computer to degrade or collapse. The situation can continue indefinitely until the user closes some running applications or the active processes free up additional virtual memory resources.

Thrashing is when the page fault and swapping happens very frequently at a higher rate, and then the operating system has to spend more time swapping these pages. This state in the operating system is known as thrashing. Because of thrashing, the CPU utilization is going to be reduced or negligible.

Techniques to Prevent Thrashing

The Local Page replacement is better than the Global Page replacement, but local page replacement has many disadvantages, so it is sometimes not helpful. Therefore below are some other techniques that are used to handle thrashing:

1. Locality Model

A locality is a set of pages that are actively used together. The locality model states that as a process executes, it moves from one locality to another. Thus, a program is generally composed of several different localities which may overlap.

For example, when a function is called, it defines a new locality where memory references are made to the function call instructions, local and global variables, etc. Similarly, when the function is exited, the process leaves this locality.

2. Working-Set Model

This model is based on the above-stated concept of the Locality Model.

The basic principle states that if we allocate enough frames to a process to accommodate its current locality, it will only fault whenever it moves to some new locality. But if the allocated frames are lesser than the size of the current locality, the process is bound to thrash.

According to this model, based on parameter A, the working set is defined as the set of pages in the most recent 'A' page references. Hence, all the actively used pages would always end up being a part of the working set.

The accuracy of the working set is dependent on the value of parameter A. If A is too large, then working sets may overlap. On the other hand, for smaller values of A, the locality might not be covered entirely.

If D is the total demand for frames and WSSi is the working set size for process i,

D = ⅀ WSSi

Now, if 'm' is the number of frames available in the memory, there are two possibilities:

    D>m, i.e., total demand exceeds the number of frames, then thrashing will occur as some processes would not get enough frames.
    D<=m, then there would be no thrashing.

If there are enough extra frames, then some more processes can be loaded into the memory. On the other hand, if the summation of working set sizes exceeds the frames' availability, some of the processes have to be suspended (swapped out of memory).

This technique prevents thrashing along with ensuring the highest degree of multiprogramming possible. Thus, it optimizes CPU utilization.

3. Page Fault Frequency

A more direct approach to handle thrashing is the one that uses the Page-Fault Frequency concept.

The problem associated with thrashing is the high page fault rate, and thus, the concept here is to control the page fault rate.

If the page fault rate is too high, it indicates that the process has too few frames allocated to it. On the contrary, a low page fault rate indicates that the process has too many frames.

Upper and lower limits can be established on the desired page fault rate, as shown in the diagram.

If the page fault rate falls below the lower limit, frames can be removed from the process. Similarly, if the page faults rate exceeds the upper limit, more frames can be allocated to the process.

In other words, the graphical state of the system should be kept limited to the rectangular region formed in the given diagram.

If the page fault rate is high with no free frames, some of the processes can be suspended and allocated to them can be reallocated to other processes. The suspended processes can restart later.

Q13.
A semaphore is an integer variable, shared among multiple processes. The main aim of using a semaphore is process synchronization and access control for a common resource in a concurrent environment.

Q14.
Process Synchronization is the coordination of execution of multiple processes in a multi-process system to ensure that they access shared resources in a controlled and predictable manner. It aims to resolve the problem of race conditions and other synchronization issues in a concurrent system.

The main objective of process synchronization is to ensure that multiple processes access shared resources without interfering with each other and to prevent the possibility of inconsistent data due to concurrent access. To achieve this, various synchronization techniques such as semaphores, monitors, and critical sections are used.

In a multi-process system, synchronization is necessary to ensure data consistency and integrity, and to avoid the risk of deadlocks and other synchronization problems. Process synchronization is an important aspect of modern operating systems, and it plays a crucial role in ensuring the correct and efficient functioning of multi-process systems.

Q15.
n interrupt is a signal emitted by hardware or software when a process or an event needs immediate attention. It alerts the processor to a high-priority process requiring interruption of the current working process. In I/O devices, one of the bus control lines is dedicated for this purpose and is called the Interrupt Service Routine (ISR).

When a device raises an interrupt at the process, the processor first completes the execution of an instruction. Then it loads the Program Counter (PC) with the address of the first instruction of the ISR. Before loading the program counter with the address, the address of the interrupted instruction is moved to a temporary location. Therefore, after handling the interrupt, the processor can continue with the process.

Q16.
when you open a file, the operating system creates an entry to represent that file and store the information about that opened file. So if there are 100 files opened in your OS then there will be 100 entries in OS (somewhere in kernel). These entries are represented by integers like (...100, 101, 102....). This entry number is the file descriptor. So it is just an integer number that uniquely represents an opened file for the process. If your process opens 10 files then your Process table will have 10 entries for file descriptors.

Similarly, when you open a network socket, it is also represented by an integer and it is called Socket Descriptor.

Q17.

Q18.
https://www.geeksforgeeks.org/difference-between-microkernel-and-monolithic-kernel/

Q19.
https://www.geeksforgeeks.org/difference-between-internal-and-external-fragmentation/

Q20.
1 I/O Devices and Controllers

An I/O device is a hardware component that can send or receive data to or from a computer system. Examples of I/O devices are keyboards, mice, printers, disks, networks, and sensors. An I/O controller is a circuit board or chip that connects an I/O device to the computer system and handles the communication between them. For example, a disk controller manages the read and write operations on a disk drive, while a network controller handles the transmission and reception of network packets.

2 I/O Ports and Buses

An I/O port is a physical or logical interface that allows an I/O device to communicate with the computer system. A physical port is a connector or socket that connects an I/O device to the computer system via a cable or a wireless link. A logical port is a software abstraction that assigns a unique address or number to an I/O device for identification and access. An I/O bus is a set of wires or channels that carries data and signals between the computer system and the I/O devices. Examples of I/O buses are PCI, USB, and Ethernet.

3 I/O Methods

An operating system can use different methods to manage I/O, depending on the speed, complexity, and type of the I/O devices and operations. Programmed I/O involves the operating system directly controlling the I/O device by issuing commands and checking the status of the device using special instructions or registers. This is simple but inefficient, as it consumes a lot of CPU time and resources. Interrupt-driven I/O is more efficient than programmed I/O, as it allows the CPU to perform other tasks while waiting for the I/O operation. However, it still involves frequent context switches and overheads. The most efficient method is direct memory access (DMA), which delegates the I/O operation to a special hardware component called the DMA controller that can transfer data between the I/O device and main memory without involving the CPU. The operating system only sets up the parameters and starts the DMA controller, and then receives an interrupt signal when the transfer is completed, thus freeing up the CPU from handling the I/O operation and reducing interrupts and context switches.

4 I/O Scheduling

An operating system can use I/O scheduling to improve the performance and fairness of I/O operations. This process involves deciding the order and priority of the I/O requests from different processes or applications. The operating system maintains a queue of pending I/O requests for each I/O device and applies a certain algorithm or policy to select the next request to be served. Common algorithms used for this purpose include First come, first served (FCFS), Shortest seek time first (SSTF), Scan, and Circular scan (C-SCAN). FCFS is simple and fair, but can cause long waiting times and low throughput for some requests. SSTF reduces the seek time and increases the throughput of the I/O device, but can cause starvation and unfairness for some requests. Scan avoids starvation and improves fairness, but can cause long waiting times and low throughput for some requests. C-SCAN avoids long waiting times and improves throughput, but can cause unfairness and waste of time for some requests.

5 I/O Buffering and Caching

An operating system can use I/O buffering and caching to enhance the performance and reliability of I/O operations. I/O buffering is the process of temporarily storing data in a memory area called a buffer before transferring it to or from an I/O device. I/O buffering can reduce the number and size of I/O operations, synchronize the speed and format of data, and handle errors and interruptions. I/O caching is the process of storing frequently accessed or recently used data in a fast memory area called a cache, which can be accessed more quickly than the main memory or the I/O device. I/O caching can reduce the latency and bandwidth of I/O operations, improve the response time and throughput of applications, and avoid redundant or unnecessary data transfers.

6 I/O Protection

An operating system can use I/O protection to ensure the security and integrity of I/O operations. This process prevents unauthorized or improper access or use of I/O devices and data by different processes or users. To do this, the operating system implements various mechanisms, such as distinguishing between user mode and kernel mode. User mode is the normal mode for running applications, while kernel mode is the privileged mode for running the operating system. The operating system can also assign different permissions or rights to different processes or users for accessing or using different I/O devices or data. Additionally, it can encrypt the data that is transferred to or from an I/O device, using a secret key or algorithm to protect the confidentiality and integrity of the data. Finally, it can decrypt the data using the same key or algorithm before delivering it to the intended process or user.

Q21.
https://www.geeksforgeeks.org/difference-between-preemptive-and-non-preemptive-cpu-scheduling-algorithms/

Q22.
Round Robin CPU Scheduling is the most important CPU Scheduling Algorithm which is ever used in the history of CPU Scheduling Algorithms. Round Robin CPU Scheduling uses Time Quantum (TQ). The Time Quantum is something which is removed from the Burst Time and lets the chunk of process to be completed.

Time Sharing is the main emphasis of the algorithm. Each step of this algorithm is carried out cyclically. The system defines a specific time slice, known as a time quantum.

First, the processes which are eligible to enter the ready queue enter the ready queue. After entering the first process in Ready Queue is executed for a Time Quantum chunk of time. After execution is complete, the process is removed from the ready queue. Even now the process requires some time to complete its execution, then the process is added to Ready Queue.

The Ready Queue does not hold processes which already present in the Ready Queue. The Ready Queue is designed in such a manner that it does not hold non unique processes. By holding same processes Redundancy of the processes increases.

After, the process execution is complete, the Ready Queue does not take the completed process for holding.

Q23.
In Priority scheduling, there is a priority number assigned to each process. In some systems, the lower the number, the higher the priority. While, in the others, the higher the number, the higher will be the priority. The Process with the higher priority among the available processes is given the CPU. There are two types of priority scheduling algorithm exists. One is Preemptive priority scheduling while the other is Non Preemptive Priority scheduling.

The priority number assigned to each of the process may or may not vary. If the priority number doesn't change itself throughout the process, it is called static priority, while if it keeps changing itself at the regular intervals, it is called dynamic priority.

Q24.
In Shortest Job Next(SJN), when choosing the next job to run, look at all the processes in the ready state and dispatch the one with the smallest service time. In this case, we need to know the service times at any given point in time. It is a non-preemptive algorithm. A new job will not be given a chance at the CPU until the current job finishes even if the new job is shorter.

Q25.
The ready queue has been partitioned into seven different queues using the multilevel queue scheduling technique. These processes are assigned to one queue based on their priority, such as memory size, process priority, or type. The method for scheduling each queue is different. Some queues are utilized for the foreground process, while others are used for the background process. The foreground queue may be scheduled using a round-robin method, and the background queue can be scheduled using an FCFS strategy.

Q26.
A Process Control Block (PCB) is a data structure that is used by an Operating System to manage and regulate how processes are carried out. In operating systems, managing the process and scheduling them properly play the most significant role in the efficient usage of memory and other system resources. In the process control block, all the details regarding the process corresponding to it like its current status, its program counter, its memory use, its open files, and details about CPU scheduling are stored.
PCB

Q27.
https://www.geeksforgeeks.org/states-of-a-process-in-operating-systems/
https://www.baeldung.com/cs/inter-process-communication

Q28.
https://www.baeldung.com/cs/inter-process-communication
